{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff08e1d",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "This notebook demonstrates the evaluation of an AI agent using the LangFuse platform. It is designed to benchmark agent performance on a golden dataset, track execution traces, and apply custom evaluation metrics.\n",
    "\n",
    "## Key Tasks\n",
    "- Connect to LangFuse and verify authentication\n",
    "- Create and manage evaluation datasets and items\n",
    "- Define and run the target agent function\n",
    "- Extract and analyze agent trajectories and context\n",
    "- Implement custom evaluation metrics for agent outputs\n",
    "- Log and score results using LangFuse's experiment tracking\n",
    "\n",
    "## Usage Type\n",
    "This notebook is intended for **research and prototyping**. It is not optimized for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b850ba",
   "metadata": {},
   "source": [
    "### LangFuse Evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install required package for Gemini API (NOT langchain-google-vertexai)\n",
    "!pip install langchain-google-genai\n",
    "\n",
    "# Install Google Generative AI package\n",
    "!pip install -q -U google-generativeai openevals\n"
   ],
   "id": "b5598b880d7bfc02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:20.301022Z",
     "start_time": "2025-11-28T15:00:20.293098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "364958d4afb887d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:20.696560Z",
     "start_time": "2025-11-28T15:00:20.692704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.getenv(\"LANGFUSE_SECRET_KEY\")"
   ],
   "id": "a0cc8fecf075e1a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-lf-efa86e37-6146-4171-a06c-74fd4ebcdd7a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f58dc07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:21.174296Z",
     "start_time": "2025-11-28T15:00:21.172028Z"
    }
   },
   "source": [
    "# Import LangFuse client library for experiment tracking\n",
    "from langfuse import get_client\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "6f120a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:21.737928Z",
     "start_time": "2025-11-28T15:00:21.706362Z"
    }
   },
   "source": [
    "# Initialize LangFuse client and verify authentication\n",
    "langfuse = get_client()\n",
    "\n",
    "# Verify connection (not recommended for production, as this is synchronous)\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "6791e68c",
   "metadata": {},
   "source": [
    "## 2. Dataset Creation and Management\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ebdc00976c20712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:22.722228Z",
     "start_time": "2025-11-28T15:00:22.720708Z"
    }
   },
   "source": [
    "# Define the name for the evaluation dataset\n",
    "dataset_name = \"golden_dataset_agent_evals_with_tools\""
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "cec4b6c1",
   "metadata": {},
   "source": [
    "#### Create new dataset items if Dataset does not Exist"
   ]
  },
  {
   "cell_type": "code",
   "id": "8c1de1e2cfc70f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:24.608600Z",
     "start_time": "2025-11-28T15:00:24.574005Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "# Create a new dataset in LangFuse for agent evaluation\n",
    "try:\n",
    "    langfuse.get_dataset(dataset_name)\n",
    "    print(\"Dataset already exists\")\n",
    "except Exception as e:\n",
    "    print(\"Dataset does not exist, Creating Dataset\")\n",
    "    langfuse.create_dataset(\n",
    "        name=dataset_name,\n",
    "        description=\"Evaluation of AI enabler data\",\n",
    "        metadata={\n",
    "            \"author\": \"UserID:Saumya\",\n",
    "            \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"type\": \"benchmark\"\n",
    "        }\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:26.034112Z",
     "start_time": "2025-11-28T15:00:25.126455Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install openpyxl",
   "id": "e487bdfcfd03a802",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (3.1.5)\r\n",
      "Requirement already satisfied: et-xmlfile in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "75deb57f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:26.113641Z",
     "start_time": "2025-11-28T15:00:26.043934Z"
    }
   },
   "source": [
    "# Load the golden dataset for evaluation from a JSON file\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"langfuse_data/golden_dataset/Ground_truth_golden_dataset.xlsx\", \"rb\") as f:\n",
    "    golden_dataset = pd.read_excel(f)\n",
    "    golden_data_list = golden_dataset.to_dict(orient=\"records\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "93cef042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:26.938216Z",
     "start_time": "2025-11-28T15:00:26.936258Z"
    }
   },
   "source": [
    "# Display the first item in the loaded golden dataset for inspection\n",
    "golden_data_list[0][\"Question\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What can i get for breakfast in Paris?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "8ba3201b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:27.372855Z",
     "start_time": "2025-11-28T15:00:27.369139Z"
    }
   },
   "source": [
    "# Check the structure and types of the data\n",
    "print(\"First item:\")\n",
    "print(golden_data_list[0])\n",
    "print(\"\\nData types:\")\n",
    "for key, value in golden_data_list[0].items():\n",
    "    print(f\"{key}: {type(value)} = {repr(value)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item:\n",
      "{'Question': 'What can i get for breakfast in Paris?', 'Answer': '\\ufeffCroissant, pain au chocolat, and café au lait', 'Document': './Paris.pdf', 'Paragraph': '§Food & Drink', 'Tool': 'retriever_tool'}\n",
      "\n",
      "Data types:\n",
      "Question: <class 'str'> = 'What can i get for breakfast in Paris?'\n",
      "Answer: <class 'str'> = '\\ufeffCroissant, pain au chocolat, and café au lait'\n",
      "Document: <class 'str'> = './Paris.pdf'\n",
      "Paragraph: <class 'str'> = '§Food & Drink'\n",
      "Tool: <class 'str'> = 'retriever_tool'\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "34892481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:27.912462Z",
     "start_time": "2025-11-28T15:00:27.876403Z"
    }
   },
   "source": [
    "# Add each item from the golden dataset to LangFuse as a dataset item\n",
    "# Check if dataset is empty before adding items\n",
    "if langfuse.get_dataset(dataset_name).items == None or len(langfuse.get_dataset(dataset_name).items) == 0:\n",
    "    print(\"Dataset is empty, adding items from golden dataset...\")\n",
    "    for item in golden_data_list:\n",
    "        # Clean the answer (remove BOM character if present)\n",
    "        answer = str(item[\"Answer\"]).lstrip('\\ufeff')\n",
    "        \n",
    "        # Convert tool string to list format for trajectory matching\n",
    "        tool_value = item[\"Tool\"]\n",
    "        trajectory = [tool_value.lower()] if isinstance(tool_value, str) else tool_value\n",
    "        \n",
    "        langfuse.create_dataset_item(\n",
    "            dataset_name=dataset_name,\n",
    "            input=str(item[\"Question\"]),\n",
    "            expected_output={\n",
    "                \"expected_answer\": answer, \n",
    "                \"trajectory\": trajectory,\n",
    "                \"golden_context\": str(item[\"Paragraph\"])\n",
    "            },\n",
    "            metadata={\n",
    "                \"agent_name\": \"Datamics Agent\",\n",
    "                \"input_type\": \"text\",\n",
    "                \"output_type\": \"text, list_trajectory\"\n",
    "            }\n",
    "        )\n",
    "else:\n",
    "    print(\"Dataset already has items, skipping addition.\")  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already has items, skipping addition.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "1313336e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:28.368233Z",
     "start_time": "2025-11-28T15:00:28.365550Z"
    }
   },
   "source": [
    "# Note: You can create new evaluation dataset items for queries with poor performance (e.g., high hallucination or low helpfulness).\n",
    "## Such traces can be filtered and added to the existing or new dataset for further analysis."
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "65aaea2e",
   "metadata": {},
   "source": [
    "## 3. Target Function Definition and Agent Invocation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "46c561a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:29.272135Z",
     "start_time": "2025-11-28T15:00:29.270343Z"
    }
   },
   "source": [
    "import os\n",
    "from graph import app\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "cea7eb83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:30.217670Z",
     "start_time": "2025-11-28T15:00:30.212746Z"
    }
   },
   "source": [
    "# Helper function to extract tool call names from agent messages\n",
    "from typing import Any, List\n",
    "\n",
    "def extract_tool_calls(messages: List[Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract tool call names from agent messages.\n",
    "    Args:\n",
    "        messages (List[Any]): List of agent messages, each may contain tool_calls.\n",
    "    Returns:\n",
    "        List[str]: List of tool call names (lowercase).\n",
    "    \"\"\"\n",
    "    tool_call_names = []\n",
    "    for message in messages:\n",
    "        # Check if message is a dict and has tool_calls\n",
    "        if isinstance(message, dict) and message.get(\"tool_calls\"):\n",
    "            tool_call_names.extend([call[\"name\"].lower() for call in message[\"tool_calls\"]])\n",
    "        # Check if message is an object with tool_calls attribute\n",
    "        elif hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "            tool_call_names.extend([call[\"name\"].lower() for call in message.tool_calls])\n",
    "    \n",
    "    return tool_call_names"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "5a243621",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "2906e3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:32.172898Z",
     "start_time": "2025-11-28T14:59:32.170045Z"
    }
   },
   "source": [
    "# Helper function to extract context documents from agent response messages\n",
    "def extract_qdrant_context(response):\n",
    "    \"\"\"\n",
    "    Extracts context documents from agent response messages.\n",
    "    Args:\n",
    "        response (dict): Agent response containing messages.\n",
    "    Returns:\n",
    "        dict: Dictionary with a list of document page contents.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    documents = []\n",
    "    for msg in response.get(\"messages\", []):\n",
    "        if (isinstance(msg, dict) and msg.get(\"name\") == \"qdrant_retriever\") or \\\n",
    "            (hasattr(msg, \"name\") and msg.name == \"qdrant_retriever\"):\n",
    "            content = msg.get(\"content\") if isinstance(msg, dict) else getattr(msg, \"content\", \"\")\n",
    "            matches = re.findall(r\"page_content='(.*?)'\", content, re.DOTALL)\n",
    "            documents.extend(matches)\n",
    "\n",
    "    return {\"documents\": documents}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "edcfbed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:32.559559Z",
     "start_time": "2025-11-28T14:59:32.556601Z"
    }
   },
   "source": [
    "# Define the application logic to be evaluated. This function is called for each dataset item.\n",
    "def target(inputs: dict, langfuse_handler) -> dict:\n",
    "    \"\"\"\n",
    "    Invokes the appliance agent with provided inputs and LangFuse callback handler.\n",
    "    Args:\n",
    "        inputs (dict): Input data for the agent (expects a 'question' key).\n",
    "        langfuse_handler: LangFuse callback handler for tracing.\n",
    "    Returns:\n",
    "        dict: Dictionary containing the agent's answer, retrieved context, and tool call trajectory.\n",
    "    \"\"\"\n",
    "    # print(\"Input question:\", inputs[\"question\"])\n",
    "    response = app.invoke(\n",
    "        inputs,\n",
    "        config={\"callbacks\":[langfuse_handler]}\n",
    "    )\n",
    "    context = extract_qdrant_context(response)\n",
    "    # return response\n",
    "    return { \"answer\": response[\"messages\"][-1].content, \"context\": context, \"trajectory\": extract_tool_calls(response[\"messages\"]) }"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fcc9b933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:34.781625Z",
     "start_time": "2025-11-28T14:59:33.036217Z"
    }
   },
   "source": [
    "# Test the target function with a sample question to verify agent response and context extraction\n",
    "target({\"messages\": \"What is famous in Paris?\"}, langfuse_handler)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': [{'type': 'text',\n",
       "   'text': \"I couldn't find information about what is famous in Paris in my internal documents. However, I found some information about Barcelona. Would you like to know what is famous in Barcelona?\",\n",
       "   'extras': {'signature': 'Ci4BcsjafKukgGYFp+w6n5sB/bQC8HGcU+URn8RUXhaA/r4r/yNam1tpT8v10cGECocBAXLI2nwjDaSBezkFYukq+/rZVTd71Yu7jH+DEXKUC5Z3yfuHA5k53DB3mcaf7unMVKYpQab9o2r/cFIgV/DlfuTqP7vl3UQTwWvyFmnEebR4oUTz7dm0+J6RpsEYAHdJa/PdTL1gA6yyX1qv2Y6dqme5Ac3O2iTYio8JMLH55rtlZHzfjun+CtwBAXLI2nwC2xbPqEvaKM7GD4XXiuyRH65rLEazCta3Or614Lrw+DqTggz/xdQ5wf//lfHe3TXKHqt6hPGIjoHyfWVJdP45mtfBeA9a87QeZ6g4ef66x5kK4z/ZI5kRTqcgItbJ/hICe3pAiEUwhNHkZL3sHoQvQAVzNqebcTnJW/vItArgzfod7GK26BE8Y55rOWrDXDPfROl4iNlCh8SY7jYq7xLUN8hT0L81N34EYA/XK2w6tLhXmXe/s/A8nViElI7bbScTZS2R99jbXDFVO1Cf939ZgMzj9hXvmQ=='},\n",
       "   'index': 0}],\n",
       " 'context': {'documents': []},\n",
       " 'trajectory': ['retriever_tool']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c93c42e5",
   "metadata": {},
   "source": [
    "## 4. Custom Evaluation Metrics and Scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdf9ed",
   "metadata": {},
   "source": [
    "#### 1. Evaluation: FINAL RESULT : LLM as judge\n",
    "Defined on LangFuse Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109abc2",
   "metadata": {},
   "source": [
    "### 2. Evaluation: Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd6e43",
   "metadata": {},
   "source": [
    "- Compares the actual sequence of steps the agent took against an expected sequence\n",
    "- Calculates a score based on how many of the expected steps were completed correctly"
   ]
  },
  {
   "cell_type": "code",
   "id": "1fcb65a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:40.446330Z",
     "start_time": "2025-11-28T14:59:40.441300Z"
    }
   },
   "source": [
    "# Custom evaluation metric: Check if agent trajectory exactly matches expected output\n",
    "def evaluate_exact_match(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate whether the agent's trajectory exactly matches the expected output.\n",
    "    Args:\n",
    "        outputs (dict): Agent output containing trajectory.\n",
    "        reference_outputs (dict): Reference output containing expected trajectory.\n",
    "    Returns:\n",
    "        dict: Result with key, score, and optional error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = outputs[\"trajectory\"] == reference_outputs[\"trajectory\"]\n",
    "        return {\n",
    "            \"key\": \"exact_match\", \n",
    "            \"score\": score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"exact_match\",\n",
    "            \"score\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Custom evaluation metric: Count unmatched steps in agent's output\n",
    "def evaluate_unmatched_steps(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the number of unmatched steps in the agent's output compared to reference.\n",
    "    Args:\n",
    "        outputs (dict): Agent output containing trajectory.\n",
    "        reference_outputs (dict): Reference output containing expected trajectory.\n",
    "    Returns:\n",
    "        dict: Result with key, score, and optional error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i = j = 0\n",
    "        unmatched_steps = 0\n",
    "\n",
    "        while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):\n",
    "            if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:\n",
    "                i += 1  # Match found, move to the next step in reference trajectory\n",
    "            else:\n",
    "                unmatched_steps += 1  # Step is not part of the reference trajectory\n",
    "            j += 1  # Always move to the next step in outputs trajectory\n",
    "\n",
    "        # Count remaining unmatched steps in outputs beyond the comparison loop\n",
    "        unmatched_steps += len(outputs['trajectory']) - j\n",
    "\n",
    "        return {\n",
    "            \"key\": \"unmatched_steps\",\n",
    "            \"score\": unmatched_steps,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"unmatched_steps\",\n",
    "            \"score\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Custom evaluation metric: Check if tool execution order matches expected trajectory\n",
    "def evaluate_tool_order(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate if the order of tool execution matches the expected trajectory order.\n",
    "    Args:\n",
    "        outputs (dict): Agent output containing trajectory.\n",
    "        reference_outputs (dict): Reference output containing expected trajectory.\n",
    "    Returns:\n",
    "        dict: Result with key, score, and optional error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        is_order_correct = outputs.get(\"trajectory\", []) == reference_outputs.get(\"trajectory\", [])\n",
    "        return {\n",
    "            \"key\": \"tool_order_correctness\",\n",
    "            \"score\": is_order_correct\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"tool_order_correctness\",\n",
    "            \"score\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "602a6c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:41.993801Z",
     "start_time": "2025-11-28T14:59:41.093840Z"
    }
   },
   "source": "",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (3.1.0)\r\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-google-genai) (1.2.0)\r\n",
      "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.9.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-google-genai) (0.9.0)\r\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.5 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-google-genai) (1.1.0)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-google-genai) (2.11.9)\r\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.28.1)\r\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.43.0)\r\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.76.0)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.26.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (5.29.5)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.70.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.32.5)\r\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.71.2)\r\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (6.2.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.4.2)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (4.9.1)\r\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from grpcio<2.0.0,>=1.33.2->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (4.15.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.33)\r\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.4.47)\r\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (25.0)\r\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (6.0.2)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (9.1.2)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.0.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.28.1)\r\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.11.4)\r\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.0.0)\r\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.25.0)\r\n",
      "Requirement already satisfied: anyio in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (2025.8.3)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.10)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.16.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.5.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.6.1)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/tomwaller/Documents/Datamics/MLConBerlin/venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.3.1)\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6d69df88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:51.317152Z",
     "start_time": "2025-11-28T14:59:51.315097Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28176145",
   "metadata": {},
   "source": [
    "### 3. Evaluation: Answer Correctness using Gemini API\n",
    "\n",
    "- Uses Gemini API to evaluate the correctness of generated answers\n",
    "- Compares the generated answer against the expected answer given the input question\n",
    "- Returns a score between 0-100"
   ]
  },
  {
   "cell_type": "code",
   "id": "94f567c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:54.011717Z",
     "start_time": "2025-11-28T14:59:52.443568Z"
    }
   },
   "source": "",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain-google-genai 3.1.0 requires google-ai-generativelanguage<1.0.0,>=0.9.0, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "23b83f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:57.701591Z",
     "start_time": "2025-11-28T14:59:57.650315Z"
    }
   },
   "source": [
    "import os\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "97fde839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:59:58.394223Z",
     "start_time": "2025-11-28T14:59:58.391146Z"
    }
   },
   "source": [
    "\n",
    "def correctness_evaluator(input : str, outputs: dict, reference_outputs: dict):\n",
    "    correctness_evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"google_genai:gemini-2.5-flash\",  # Correct model name for Google AI Studio\n",
    "        feedback_key=\"correctness\",\n",
    "    )\n",
    "\n",
    "    eval_result = correctness_evaluator(\n",
    "        inputs=input,\n",
    "        outputs=outputs[\"answer\"],\n",
    "        reference_outputs=reference_outputs[\"expected_answer\"]\n",
    "    )\n",
    "    return eval_result"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "d9020c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:02.156469Z",
     "start_time": "2025-11-28T14:59:59.156880Z"
    }
   },
   "source": [
    "output = target({\"messages\": \"what is there in budapest\"}, langfuse_handler)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "c34bdddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:05.565317Z",
     "start_time": "2025-11-28T15:00:03.918638Z"
    }
   },
   "source": [
    "correctness_evaluator(\n",
    "        input= \"what is there in budapest\",\n",
    "        outputs = output, \n",
    "        reference_outputs = {\"expected_answer\": \"Budapest is known for its stunning architecture, thermal baths, the Danube River, Buda Castle, and vibrant cultural scene.\"})"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'key': 'correctness',\n",
       " 'score': False,\n",
       " 'comment': \"The model failed to provide any information about Budapest, stating it couldn't find specific information. Instead, it offered information about Barcelona, which is not what the user asked for. The answer is incomplete and does not address the question asked by the user. Thus, the score should be: False.\",\n",
       " 'metadata': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "252b9937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:39.419235Z",
     "start_time": "2025-11-28T15:00:39.415934Z"
    }
   },
   "source": [
    "# Prepare metadata for experiment tracking and reproducibility\n",
    "metadata = {\n",
    "    \"llm_model_name\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"embedding_model_name\": os.getenv(\"EMBEDDING_MODEL_NAME\"),\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"qdrant_url\": os.getenv(\"QDRANT_URL\"),\n",
    "    \"collection_name\": os.getenv(\"COLLECTION_NAME\"),\n",
    "    \"chunking_strategy\": {\"chunk_size\": 150, \"chunk_overlap\": 64},\n",
    "}\n",
    "\n",
    "# Define experiment prefix for logging and organization\n",
    "# experiment_prefix = f\"agent_eval_{prompt_detail['agent_system_prompt']}_{prompt_detail['router_prompt']}_{prompt_detail['user_prompt']}\"\n",
    "run_prefix = f\"agent_eval_{os.getenv(\"COLLECTION_NAME\")}_{os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")}\""
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "3fd9e3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:40.120825Z",
     "start_time": "2025-11-28T15:00:40.117274Z"
    }
   },
   "source": [
    "metadata, run_prefix"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'llm_model_name': None,\n",
       "  'embedding_model_name': '“huggingface-sentence-transformers/all-MiniLM-L6-v2\"',\n",
       "  'dataset_name': 'golden_dataset_agent_evals_with_tools',\n",
       "  'qdrant_url': 'http://localhost:6333',\n",
       "  'collection_name': 'locations_pdfs',\n",
       "  'chunking_strategy': {'chunk_size': 150, 'chunk_overlap': 64}},\n",
       " 'agent_eval_locations_pdfs_None')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "892285a5",
   "metadata": {},
   "source": [
    "## 5. Experiment Execution and Results Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "38f0ec08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:47.692006Z",
     "start_time": "2025-11-28T15:00:47.666955Z"
    }
   },
   "source": [
    "# Load the evaluation dataset from LangFuse for experiment execution\n",
    "from langfuse import get_client\n",
    "\n",
    "dataset = get_client().get_dataset(dataset_name)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "0543c1d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:48.442970Z",
     "start_time": "2025-11-28T15:00:48.440323Z"
    }
   },
   "source": [
    "# Define run names for experiment runs (can be extended for multiple runs)\n",
    "run_names = [\"demo_run\"]"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "c7ebef26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:00:48.953251Z",
     "start_time": "2025-11-28T15:00:48.949913Z"
    }
   },
   "source": [
    "# Print input and expected output for the first dataset item to verify data integrity\n",
    "for item in dataset.items:\n",
    "    print(item.input)\n",
    "    print(item.expected_output)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National Holidays in Austria\n",
      "{'trajectory': ['generic'], 'golden_context': 'nan', 'expected_answer': 'List of National holidays in Austria'}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "8720e612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:07:17.947872Z",
     "start_time": "2025-11-28T15:00:49.689551Z"
    }
   },
   "source": [
    "import time\n",
    "# Run the experiment for each dataset item and log results to LangFuse\n",
    "for item in dataset.items:\n",
    "    # Use the item.run() context manager for automatic trace linking\n",
    "    with item.run(\n",
    "        run_name=run_prefix,\n",
    "        run_description=\"Demo AI Agent Evaluation\",\n",
    "        run_metadata=metadata\n",
    "    ) as root_span:\n",
    "\n",
    "        # Invoke the target agent function and get output\n",
    "        output = target({\"messages\": item.input}, langfuse_handler)\n",
    "        root_span.update(output=output, input=item.input, metadata=metadata)\n",
    "\n",
    "        time.sleep(30)  ## adding delays \n",
    "        # Track custom trajectory evaluations\n",
    "        evaluate_unmatched_steps_dict = evaluate_unmatched_steps(output, item.expected_output)\n",
    "        evaluate_exact_match_dict = evaluate_exact_match(output, item.expected_output)\n",
    "        evaluate_tool_order_dict = evaluate_tool_order(output, item.expected_output) \n",
    "        evaluate_correctness = correctness_evaluator(item.input, output, item.expected_output )\n",
    "        \n",
    "        # Log evaluation scores to LangFuse\n",
    "        root_span.score_trace(\n",
    "            name=evaluate_unmatched_steps_dict[\"key\"],\n",
    "            value=evaluate_unmatched_steps_dict[\"score\"],\n",
    "        )       \n",
    "\n",
    "        root_span.score_trace(\n",
    "            name=evaluate_exact_match_dict[\"key\"],\n",
    "            value=evaluate_exact_match_dict[\"score\"],\n",
    "        )\n",
    "\n",
    "        root_span.score_trace(\n",
    "            name=evaluate_tool_order_dict[\"key\"],\n",
    "            value=evaluate_tool_order_dict[\"score\"],\n",
    "            comment=evaluate_tool_order_dict.get(\"error\", \"check tool order correctness\")\n",
    "        )\n",
    "\n",
    "        root_span.score_trace(\n",
    "            name=evaluate_correctness[\"key\"],\n",
    "            value=evaluate_correctness[\"score\"],\n",
    "            comment=evaluate_correctness[\"comment\"]\n",
    "        )\n",
    "\n",
    "        # Optionally, score the result against the expected output\n",
    "        # root_span.score_trace(\n",
    "        #     name=\"user-feedback_test\",\n",
    "        #     value=1,\n",
    "        #     comment=\"This is a test comment\", \n",
    "        # )\n",
    "        \n",
    "        \n",
    "# Ensure all data is sent to the LangFuse server at the end of the experiment\n",
    "langfuse.flush()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "6cb0b7ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:07:18.087318Z",
     "start_time": "2025-11-28T15:07:18.085678Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be33e74659ca21cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
